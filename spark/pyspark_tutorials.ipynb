{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to work with the basics of Spark by creating RDDs and performing basic operations on them. You will aso learn to do operations of spark dataframe and querry using SparkSQL. Finally, two examples utilizing Spark Mllib are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations: wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are couting how many times each word appears in a file called README.md. The fisrt step is to create a RDD from the data file called README.md. We will do some simple operations like count, take, collect on the RDD. Then we will use transfomations like filter, flatmap and map to get the wordcount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"/users/PZS0645/support/workshop/Bigdata/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a RDD is created, we can do operations on the RDD.\n",
    "For example, count the number of lines of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what’s in the RDD\n",
    "data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first command shows the first three lines (each line is preceded by the letter u)of RDD while the second shows the entire file. We should be cautious with collect() function when data size is large as it requires a large amount of memory allocated  for the driver node to collect entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data type\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’ll do a simple transformation: filter all the lines with “Spark” in them and count such lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithSpark = data.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “filter” function finds lines with “Spark” in them and saves that to the variable “linesWithSpark”.\n",
    "We can then use the function “count” to display the number of lines. Here the function *filter* is a tranformation and *count* is a action operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll combine those two commands into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter(lambda line: \"Spark\" in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our final interactive example, we’ll show how to count the number of times a word appears in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wordCounts = data.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be a screen of data that shows you each word in the file and how many times it appeared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spark DataFrames\n",
    "\n",
    "\n",
    "For these examples, we just need to import two pyspark.sql libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *  # Necessary for creating schemas\n",
    "from pyspark.sql.functions import * # Importing PySpark functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Simple DataFrame from a Tuple List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a tuple list\n",
    "a_list = [('a', 1), ('b', 2), ('c', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame, without supplying a schema value\n",
    "df_from_list_no_schema = \\\n",
    "sqlContext.createDataFrame(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DF object\n",
    "print (df_from_list_no_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a collected list of Row objects\n",
    "print (df_from_list_no_schema.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame\n",
    "df_from_list_no_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Simple DataFrame from a Tuple List and a Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame, this time with schema\n",
    "nice_df = sqlContext.createDataFrame(a_list, ['letters', 'numbers']) # this simple schema contains just column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame\n",
    "nice_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame's schema\n",
    "nice_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Inspection Functions\n",
    "\n",
    "We now have a nice_df, here are some nice functions for inspecting the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `columns`: return all column names as a list\n",
    "nice_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `dtypes`: get the datatypes for all columns\n",
    "nice_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `printSchema()`: prints the schema of the supplied DF\n",
    "nice_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `first()` returns the first row as a Row while\n",
    "# `head()` and `take()` return `n` number of Row objects\n",
    "print (nice_df.first()) # can't supply a value; never a list\n",
    "print (nice_df.head(2)) # can optionally supply a value (default: 1);\n",
    "                      # with n > 1, a list\n",
    "print (nice_df.take(2)) # expects a value; always a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `count()`: returns a count of all rows in DF\n",
    "nice_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `describe()`: print out stats for numerical columns\n",
    "nice_df.describe().show() # can optionally supply a list of column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatively Simple DataFrame Manipulation Functions\n",
    "Let's use these functions:\n",
    "\n",
    "unionAll(): combine two DataFrames together\n",
    "orderBy(): perform sorting of DataFrame columns\n",
    "select(): select which DataFrame columns to retain\n",
    "drop(): select a single DataFrame column to remove\n",
    "filter(): retain DataFrame rows that match a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the DataFrame and add it to itself\n",
    "(nice_df\n",
    " .unionAll(nice_df)\n",
    " .show())\n",
    "\n",
    "# Add it to itself twice\n",
    "(nice_df\n",
    " .unionAll(nice_df)\n",
    " .unionAll(nice_df)\n",
    " .show())\n",
    "\n",
    "# Coercion will occur if schemas don't align\n",
    "(nice_df\n",
    " .select(['numbers', 'letters'])\n",
    " .unionAll(nice_df)\n",
    " .show())\n",
    "\n",
    "(nice_df\n",
    " .select(['numbers', 'letters'])\n",
    " .unionAll(nice_df)\n",
    " .printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sorting the DataFrame by the `numbers` column\n",
    "(nice_df\n",
    " .unionAll(nice_df)\n",
    " .unionAll(nice_df)\n",
    " .orderBy('numbers')\n",
    " .show())\n",
    "\n",
    "# Sort the same column in reverse order\n",
    "(nice_df\n",
    " .unionAll(nice_df)\n",
    " .unionAll(nice_df)\n",
    " .orderBy('numbers',\n",
    "          ascending = False)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `select()` and `drop()` both take a list of column names\n",
    "# and these functions do exactly what you might expect\n",
    "\n",
    "# Select only the first column of the DF\n",
    "(nice_df\n",
    " .select('letters')\n",
    " .show())\n",
    "\n",
    "# Re-order columns in the DF using `select()`\n",
    "(nice_df\n",
    " .select(['numbers', 'letters'])\n",
    " .show())\n",
    "\n",
    "# Drop the second column of the DF\n",
    "(nice_df\n",
    " .drop('letters')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `filter()` function performs filtering of DF rows\n",
    "\n",
    "# Here is some numeric filtering with comparison operators\n",
    "# (>, <, >=, <=, ==, != all work)\n",
    "\n",
    "# Filter rows where values in `numbers` is > 1\n",
    "(nice_df\n",
    " .filter(nice_df.numbers > 1)\n",
    " .show())\n",
    "\n",
    "# Perform two filter operations\n",
    "(nice_df\n",
    " .filter(nice_df.numbers > 1)\n",
    " .filter(nice_df.numbers < 3)\n",
    " .show())\n",
    "\n",
    "# Not just numbers! Use the `filter()` + `isin()`\n",
    "# combo to filter on string columns with a set of values\n",
    "(nice_df\n",
    " .filter(nice_df.letters\n",
    "         .isin(['a', 'b']))\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will load data from a csv file, create a dataframe and then perform operations on it. You can read more about the data here, http://kdd.ics.uci.edu/databases/kddcup99/kddcup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.csv(\"/users/PZS0645/support/workshop/Bigdata/data.csv\", header='TRUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the schema of the data\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the schema of the data\n",
    "data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out first 5 records as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the schema of the data\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two feilds and only show those.\n",
    "data.select(\"dst_bytes\",\"flag\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two feilds and only show those.\n",
    "data.filter(data.flag!=\"SF\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groub data by a column and operations on grouped data\n",
    "data.select(\"protocal_type\", \"duration\", \"dst_bytes\").groupBy(\"protocal_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter and groupBy\n",
    "data.select(\"protocal_type\", \"duration\", \"dst_bytes\").filter(data.duration>1000).filter(data.dst_bytes==0).groupBy(\"protocal_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL querries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to run SparkSQL querries, we have to register the dataframe as table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.registerTempTable(\"interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can querry on the table called *interactions* based on conditions. For example, select tcp network interactions with more than 1 second duration and no transfer from destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp = sqlContext.sql(\" SELECT duration, dst_bytes FROM interactions WHERE protocal_type ='tcp' AND duration>1000 AND dst_bytes=0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Mllib examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use Spark's machine learning library MLlib to build a Logistic Regression classifier for network attack detection. We will use the complete KDD Cup 1999 datasets (http://kdd.ics.uci.edu/databases/kddcup99/kddcup99) in order to test Spark capabilities with large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training data\n",
    "import urllib.request\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a RDD for training data\n",
    "data_file = \"./kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "print (\"Train data size is {}\".format(raw_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import test data\n",
    "ft = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"corrected.gz\")\n",
    "\n",
    "\n",
    "test_data_file = \"./corrected.gz\"\n",
    "test_raw_data = sc.textFile(test_data_file)\n",
    "\n",
    "print (\"Test data size is {}\".format(test_raw_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing training data \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "\n",
    "def parse_interaction(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # leave_out = [1,2,3,41]\n",
    "    clean_line_split = line_split[0:1]+line_split[4:41]\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
    "\n",
    "training_data = raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing test data\n",
    "test_data = test_raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is widely used to predict a binary response. Spark implements two algorithms to solve logistic regression: mini-batch gradient descent and L-BFGS. L-BFGS is recommended over mini-batch gradient descent for faster convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a classifier\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from time import time\n",
    "\n",
    "# Build the model\n",
    "logit_model = LogisticRegressionWithLBFGS.train(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on new data\n",
    "In order to measure the classification error on our test data, we use map on the test_data RDD and the model to predict each test point class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_and_preds = test_data.map(lambda p: (p.label, logit_model.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_and_preds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification results are returned in pars, with the actual test label and the predicted one. This is used to calculate the classification error by using filter and count as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_accuracy = labels_and_preds.filter(lambda vp:vp[0] == vp[1]).count() / float(test_data.count())\n",
    "\n",
    "\n",
    "print (\"Test accuracy is {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/usr/local/spark/2.3.0/spark-2.3.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of our workshop! You can save your work by clicking on *File* and *save* and then use it later. Once your are done with the tutorial, you can click on *Logout* on the top right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## References\n",
    "\n",
    "http://spark.apache.org The main reference for PySpark is:\n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/index.html These examples are available at:\n",
    "\n",
    "https://github.com/rich-iannone/so-many-pyspark-examples Information on the Parquet file format can be found at its project page:\n",
    "\n",
    "http://parquet.apache.org The GitHub project page for spark-csv package; contains usage documentation:\n",
    "\n",
    "https://github.com/databricks/spark-csv\n",
    "\n",
    "https://www.codementor.io/jadianes/spark-mllib-logistic-regression-du107neto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get spark node numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._jsc.sc().getExecutorMemoryStatus().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._jsc.sc().getExecutorMemoryStatus().keySet().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
